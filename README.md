# machine_learning_project-unsupervised-learning

## Project Outcomes
- Unsupervised Learning: perform unsupervised learning techniques on a wholesale data dataset. The project involves four main parts: exploratory data analysis and pre-processing, KMeans clustering, hierarchical clustering, and PCA.
### Duration:
Approximately 1 hour and 40 minutes
### Project Description:
In this project, we will apply unsupervised learning techniques to a real-world data set and use data visualization tools to communicate the insights gained from the analysis.

The data set for this project is the "Wholesale Data" dataset containing information about various products sold by a grocery store.
The project will involve the following tasks:

-	Exploratory data analysis and pre-processing: We will import and clean the data sets, analyze and visualize the relationships between the different variables, handle missing values and outliers, and perform feature engineering as needed.
-	Unsupervised learning: We will use the Wholesale Data dataset to perform k-means clustering, hierarchical clustering, and principal component analysis (PCA) to identify patterns and group similar data points together. We will determine the optimal number of clusters and communicate the insights gained through data visualization.

The ultimate goal of the project is to gain insights from the data sets and communicate these insights to stakeholders using appropriate visualizations and metrics to make informed decisions based on the business questions asked."


Outcome: 

K-Means Clustering: The silhouette score for K-Means clustering with four clusters is approximately 0.262, indicating a fair amount of separation between clusters

Hierarchical Clustering Performance: Hierarchical clustering with five clusters yielded a silhouette score of approximately 0.233, which is lower than K-Means, suggesting less distinction between the clusters

Comparative Cluster Quality: While both clustering methods achieved positive silhouette scores, indicating that on average, samples are closer to their own clusters than to others, K-Means clustering performed slightly better according to this metric

Dominant First Component: The first principal component alone accounts for approximately 44.8% of the variance, indicating a strong underlying factor or combination of features

